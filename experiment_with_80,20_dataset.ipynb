{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment with 80,20 dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iRcaJECxPL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c2fe418f-128c-4026-c643-77b0c9489a41"
      },
      "source": [
        "import pandas as pd\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "train='https://drive.google.com/open?id=1Mskhv7GETMD_buUAJtEsKGFidJg87A2X'\n",
        "test='https://drive.google.com/open?id=12fZDV8QRBlhQoLgbjiACyc5ypIcOytk5'\n",
        "'''validate='https://drive.google.com/open?id=1NwHUWbbH3rxGCDBJZ7v6f86Ky9-Ve72A'''\n",
        "\n",
        "fluff, id = train.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.csv')  \n",
        "df1 = pd.read_csv('train.csv')\n",
        "\n",
        "fluff, id = test.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('test.csv')  \n",
        "df2 = pd.read_csv('test.csv')\n",
        "\n",
        "'''fluff, id = validate.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('validate.csv')  \n",
        "df3 = pd.read_csv('validate.csv')'''"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1Mskhv7GETMD_buUAJtEsKGFidJg87A2X\n",
            "12fZDV8QRBlhQoLgbjiACyc5ypIcOytk5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"fluff, id = validate.split('=')\\nprint (id) # Verify that you have everything after '='\\ndownloaded = drive.CreateFile({'id':id}) \\ndownloaded.GetContentFile('validate.csv')  \\ndf3 = pd.read_csv('validate.csv')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmMqcC5X0z22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "883828c4-e029-4aa0-9722-dc22ef55bdf1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 3000\n",
        "embedding_dim = 150\n",
        "#vocab_size = 5000\n",
        "#embedding_dim = 150\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "sentences1 = []\n",
        "labels1 = []\n",
        "for sen1 in df1['Document_stp4']:\n",
        "     sentences1.append((sen1))\n",
        "for lab1 in df1['label']:\n",
        "  if lab1=='male':\n",
        "      labels1.append(0)\n",
        "  elif lab1=='female':\n",
        "      labels1.append(1)\n",
        "sentences1=[str(s) for s in sentences1]   \n",
        "#labels=[str(l) for l in labels]   \n",
        "print(len(sentences1),len(labels1))\n",
        "\n",
        "sentences2 = []\n",
        "labels2 = []\n",
        "for sen2 in df2['Document_stp4']:\n",
        "     sentences2.append((sen2))\n",
        "for lab2 in df2['label']:\n",
        "  if lab2=='male':\n",
        "      labels2.append(0)\n",
        "  elif lab2=='female':\n",
        "      labels2.append(1)\n",
        "sentences2=[str(s) for s in sentences2]   \n",
        "#labels=[str(l) for l in labels]   \n",
        "print(len(sentences2),len(labels2))\n",
        "'''sentences3 = []\n",
        "labels3 = []\n",
        "for sen3 in df3['Document_stp4']:\n",
        "     sentences3.append((sen3))\n",
        "for lab3 in df3['label']:\n",
        "  if lab3=='male':\n",
        "      labels3.append(0)\n",
        "  elif lab3=='female':\n",
        "      labels3.append(1)\n",
        "sentences3=[str(s) for s in sentences3]   \n",
        "#labels=[str(l) for l in labels]   \n",
        "print(len(sentences3),len(labels3))'''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164800 164800\n",
            "41200 41200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"sentences3 = []\\nlabels3 = []\\nfor sen3 in df3['Document_stp4']:\\n     sentences3.append((sen3))\\nfor lab3 in df3['label']:\\n  if lab3=='male':\\n      labels3.append(0)\\n  elif lab3=='female':\\n      labels3.append(1)\\nsentences3=[str(s) for s in sentences3]   \\n#labels=[str(l) for l in labels]   \\nprint(len(sentences3),len(labels3))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlufoP12b1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "23a28ed9-786f-4ec9-f2d1-7f910719436f"
      },
      "source": [
        "from numpy import array #to convert list of string to numpy array\n",
        "training_sentences = array(sentences1)\n",
        "testing_sentences = array(sentences2)\n",
        "training_labels = array(labels1)\n",
        "testing_labels = array(labels2)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "#print(len(sentences))\n",
        "#print(training_padded.shape)\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "#print(testing_padded.shape)\n",
        "print(training_labels)\n",
        "print(len(word_index))\n",
        "print(training_sentences.shape)\n",
        "print(testing_labels.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 ... 0 0 0]\n",
            "202421\n",
            "(164800,)\n",
            "(41200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiwL5biA9L7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZmGk_xd23bV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "98b702bd-d677-4161-8e5f-8d33b477f434"
      },
      "source": [
        "'''model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    \n",
        "    #(tf.keras.layers.Embedding(vocab_size, 64),\n",
        "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    #tf.keras.layers.Dense(64, activation='relu'),\n",
        "    #tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "    #tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    #tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    #tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    #tf.keras.layers.Dense(24, activation='relu'),\n",
        "    #tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])'''\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 120, 150)          450000    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 64)                46848     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 501,073\n",
            "Trainable params: 501,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cnno7fC3GJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "c0e58f51-89cc-42d4-940d-614510541823"
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)\n",
        "score = model.evaluate(training_labels, testing_labels, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5150/5150 [==============================] - 465s 90ms/step - loss: 0.5817 - accuracy: 0.6654 - val_loss: 0.5619 - val_accuracy: 0.6827\n",
            "Epoch 2/5\n",
            "5150/5150 [==============================] - 463s 90ms/step - loss: 0.5435 - accuracy: 0.6955 - val_loss: 0.5613 - val_accuracy: 0.6826\n",
            "Epoch 3/5\n",
            " 252/5150 [>.............................] - ETA: 6:52 - loss: 0.5073 - accuracy: 0.7243"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw8RfnQSA75G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'acc')\n",
        "plot_graphs(history, 'loss')\n",
        "\n",
        "model.save(\"test.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tXmRdD19PFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}